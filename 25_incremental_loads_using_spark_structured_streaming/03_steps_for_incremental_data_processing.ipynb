{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "887094df-31f0-4f8e-8f0b-6a94fff72b0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Steps for Incremental Data Processing\n",
    "\n",
    "Let us get into the steps to read JSON data from the underlying file system and then write to `parquet` file format incrementally using Spark Structured Streaming. Here is what we are going to do for the same.\n",
    "\n",
    "* Ensure a working directory is created in HDFS.\n",
    "* Get the GHArchive files which are in compressed JSON format from the source to HDFS.\n",
    "* Read the JSON files using `readStream` by inferring the schema. It will create a Dataframe object with `isStreaming` set to true.\n",
    "* Write the Dataframe to Parquet file format using `trigger(once=True)`. We can also use fixed interval micro batches to run the job more frequently. Some of the features will be different (for example, `maxFilesPerTrigger` is not applicable `trigger(once=True)`.\n",
    "* Validate whether the data is populated in parquet file format or not.\n",
    "* Add new JSON files in the source location.\n",
    "* Run the process once again and validate.\n",
    "\n",
    "> By the end of it we will be able to convert the JSON files to Parquet file format incrementally. In contrast to traditional pipelines, the checkpoint will be managed by Spark Structured Streaming framework. In traditional batch pipelines, we need to take care of this checkpoint (also known as bookmark or marker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7cc68c2f-9d31-4182-857a-fc07e98a4381",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03 Steps for Incremental Data Processing",
   "notebookOrigID": 2400095431327527,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
