{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42c8cb6",
   "metadata": {},
   "source": [
    "## Reading Web Server logs using Spark Structured Streaming\n",
    "\n",
    "As we are ready with the ability to simulate log message generation, let us get into reading these logs using Spark Structured Streaming.\n",
    "* `spark` which is of type `SparkSession` have an attribute called as `readStream`. It is of type `pyspark.sql.streaming.DataStreamReader`.\n",
    "* It exposes APIs such as `csv`, `json`, etc along with `format`. To read data from web servers, we can use `socket` as format.\n",
    "* We need to set options `host` and `port`, then invoke `load` to read data in streaming fashion.\n",
    "* It will create an object which will be of type `pyspark.sql.dataframe.DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d22977",
   "metadata": {},
   "source": [
    "Launch Pyspark using below commands and run Spark Structured Streaming Code.\n",
    "\n",
    "**Using Pyspark2**\n",
    "\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark3**\n",
    "\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "\n",
    "pyspark3 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead7099",
   "metadata": {},
   "source": [
    "* Deriving the hostname.\n",
    "\n",
    "```python\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "```\n",
    "\n",
    "* Creating Streaming Data Frame.\n",
    "\n",
    "```python\n",
    "socketDF = spark. \\\n",
    "    readStream. \\\n",
    "    format(\"socket\"). \\\n",
    "    option(\"host\", hostname). \\\n",
    "    option(\"port\", 9000). \\\n",
    "    load()\n",
    "```\n",
    "\n",
    "* Validating whether the data frame is streaming data frame or not.\n",
    "\n",
    "```python\n",
    "socketDF.isStreaming\n",
    "```\n",
    "\n",
    "* Previewing the schema.\n",
    "\n",
    "```python\n",
    "socketDF.printSchema()\n",
    "```\n",
    "\n",
    "* We cannot use `show` to preview the data for streaming data frame.\n",
    "\n",
    "```python\n",
    "socketDF.show() # throws exceptions\n",
    "```\n",
    "\n",
    "* Previewing the data. It will run continuously.\n",
    "\n",
    "```python\n",
    "socketDF. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"append\"). \\\n",
    "    format(\"console\"). \\\n",
    "    start()\n",
    "```\n",
    "\n",
    "* Run the below code and watch the output. You will see messages being processed every 5 seconds.\n",
    "\n",
    "```python\n",
    "socketDF. \\\n",
    "    writeStream. \\\n",
    "    outputMode(\"append\"). \\\n",
    "    format(\"console\"). \\\n",
    "    trigger(processingTime='5 seconds'). \\\n",
    "    start()\n",
    "\n",
    "# Triggers every 5 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb040fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
